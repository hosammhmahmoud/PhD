{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2065/2065 [16:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully created with 32982 rows.\n",
      "Data successfully loaded to table 'silken-mile-379810.libmev_dataset.searchers_bundles'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_CREDENTIALS_PATH = os.getenv(\"GOOGLE_CREDENTIALS_PATH\")\n",
    "BQ_DATASET_ID = os.getenv(\"BQ_DATASET_ID\")\n",
    "BQ_TABLE_ID = os.getenv(\"BQ_TABLE_ID\")\n",
    "\n",
    "\n",
    "\n",
    "def load_to_table(df, dataset_id=BQ_DATASET_ID, table_id=BQ_TABLE_ID, credentials_path=GOOGLE_CREDENTIALS_PATH):\n",
    "    \"\"\"\n",
    "    Uploads a pandas DataFrame to a Google BigQuery table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "        table_id = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result() \n",
    "\n",
    "        print(f\"Data successfully loaded to table '{table_id}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to BigQuery: {e}\")\n",
    "        print(f\"Failed to add data to table '{table_id}'.\")\n",
    "\n",
    "\n",
    "def get_libmev_data(start_date, end_date, interval_minutes=5):\n",
    "    \"\"\"\n",
    "    Fetches data from the libMEV API within a specified date range.\n",
    "    Parameters:\n",
    "        start_date (int): Start timestamp in Unix epoch seconds.\n",
    "        end_date (int): End timestamp in Unix epoch seconds.\n",
    "        interval_minutes (int): Interval for API requests in minutes (default: 5 minutes)\n",
    "    Returns: pd.DataFrame: A DataFrame containing the concatenated data from the API.\n",
    "    \"\"\"\n",
    "    df_final = pd.DataFrame()\n",
    "    interval_seconds = interval_minutes * 60\n",
    "    total_intervals = (end_date - start_date) // interval_seconds\n",
    "\n",
    "\n",
    "    for i in tqdm(range(total_intervals)):\n",
    "        try:\n",
    "            \n",
    "            url = f\"https://api.libmev.com/v1/bundles?timestampRange={end_date - interval_seconds},{end_date}\"\n",
    "\n",
    "            # Fetch and append data\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an error for HTTP issues\n",
    "\n",
    "            data = response.json().get(\"data\", [])\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                df_final = pd.concat([df_final, df], ignore_index=True)\n",
    "\n",
    "            # Update the end_date for the next iteration\n",
    "            end_date -= interval_seconds\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error at timestamp {end_date}: {e}\")\n",
    "\n",
    "    # Filter, clean, and preprocess the data\n",
    "    df_final =  df_final.loc[df_final.timestamp > start_date]\n",
    "    df_final.index = pd.to_datetime(df_final.timestamp, unit= 's')\n",
    "    df_final.drop('tokens', axis=1, inplace=True) \n",
    "    df_final.drop('token_balance_delta', axis=1, inplace=True) \n",
    "    df_final.rename_axis('DATETIME', inplace = True)\n",
    "\n",
    "    print(f\"DataFrame successfully created with {len(df_final)} rows.\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def get_BQ(query, credentials_path=GOOGLE_CREDENTIALS_PATH):\n",
    "    \"\"\"\n",
    "    Fetches data from BigQuery using a SQL query.\n",
    "    Parameters: query (str): The SQL query to execute.\n",
    "    Returns: pd.DataFrame: The resulting DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a BigQuery engine using SQLAlchemy\n",
    "        bq_engine = create_engine(\"bigquery://\", credentials_path=credentials_path)\n",
    "        df = pd.read_sql(query, bq_engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from BigQuery: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_timestamp = get_BQ(\n",
    "        \"\"\"\n",
    "        SELECT MAX(TIMESTAMP) AS max_timestamp\n",
    "        FROM `silken-mile-379810.libmev_dataset.searchers_bundles`\n",
    "        \"\"\")\n",
    "    \n",
    "    start_timestamp = int(start_timestamp.iloc[0])\n",
    "    \n",
    "    end_timestamp = int(datetime.datetime.now().timestamp())  \n",
    "\n",
    "    # Fetch data from libMEV\n",
    "    df_libmev = get_libmev_data(start_timestamp, end_timestamp)\n",
    "\n",
    "    # Load data into BigQuery\n",
    "    load_to_table(df_libmev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
